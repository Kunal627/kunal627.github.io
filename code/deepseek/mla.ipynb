{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Multi-head Latent Attention\n",
    "\n",
    "I will not rant about what is deepseek, which you can find at their website https://deepseek.ai/. What caught my attention was the Multi-Head latent attention used in the model instead \n",
    "of the traditional attention mechanism.\n",
    "\n",
    "Multi-Head Latent Attention (MHLA) is a key innovation in DeepSeek V2, enhancing the model’s ability to capture complex dependencies while maintaining efficiency. Unlike traditional self-attention, which directly operates on token interactions, MHLA introduces a latent space where multiple attention heads process compressed representations. This reduces computational overhead while preserving expressivity, making it particularly effective for scaling large models. By leveraging this approach, DeepSeek V2 achieves better efficiency-memory trade-offs, enabling faster inference and stronger long-range reasoning—a crucial factor for tasks requiring deep contextual understanding.  \n",
    "\n",
    "In this post i would take a stab at implementing the MHLA using pytorch. It took me some time to decipher the maths and the tensor shapes but i think i have good grasp of it now. I would refer the paper [Deepseek v2 paper](https://arxiv.org/pdf/2405.04434) from Deepseek AI.\n",
    "\n",
    "The following images are from the paper which illustrates the MLHA and the math behind it:\n",
    "\n",
    "![Multi-head latent attention](../../images/mla.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![maths behind MHLA](../../images/mla-formulas.JPG)\n",
    "\n",
    "\n",
    "#### Notes on symbols and variables used in the deepseek v2 paper\n",
    "\n",
    "All the dimensions are only for one token where t in subscript represents token t. The batch and sequence length are omitted for the sake of simplicity.\n",
    "\n",
    "| symbol | dimensions| remarks|\n",
    "|--------|----------|-----------|\n",
    "| $h_t$  |  d       | same as model dim or dmodel used in transformers. These are the embedding dimensions = $d_h$ x $n_h$|\n",
    "| $d_h$  |          | number of dimensions per attention head |\n",
    "| $n_h$  |          | number of attention heads|\n",
    "| $d_c$  |          | KV compression dimension|\n",
    "| $d_c^{'}$ |       | dimensions for compressed query where  $d_c^{'}$  (<<$d_h$ x $n_h$) |\n",
    "| $C_t^{KV}$| $d_c$ | compressed latent vector for keys and values where $d_c$ (<<$d_h$ x $n_h$)|\n",
    "| $W^{DKV}$ | $d_c$ x d | down projection matrix |\n",
    "| $W^{UK}$  | d x $d_c$ | up projection matrix for keys|\n",
    "| $W^{UV}$  | d x $d_c$ | up projection matrix for values|\n",
    "| $W^{KR}$  | $d_h^R$ x d | matrix to produce the decouples keys|\n",
    "| $K_t^R$   | $d_h^R$     | shared key to carry RoPE |\n",
    "| $W^{DQ}$  | $d_c^{'}$ x d| down projection matrix for queries |\n",
    "| $C_t^Q$   | $d_c^{'}$    | compressed laten vector for queries| \n",
    "| $W^{UQ}$  | d x $d_c^{'}$ | up projection matrix for queries |\n",
    "| $q_t^C$   |   d           | queries after up projection|\n",
    "| $W^{QR}$  | $d_h^R$ x  $d_c^{'}$ | matrix to produce the decouples queries|\n",
    "| $q_t{R}$   | $d_h^R$         | multihead queries to carry RoPE|\n",
    "\n",
    "\n",
    "After concatenating rope encodings with queries and keys, the new dimensions become $d_h$ + $d_h^R$. \n",
    "\n",
    "I have implemented both traditional multi head self attention and MLHA in pytorch below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # number of KV cache elements per token per layer 2048\n",
      "torch.Size([4, 10, 1024])\n"
     ]
    }
   ],
   "source": [
    "# original attention mechanism from vaswani et al.\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "n_embed = 1024\n",
    "n_heads = 16\n",
    "d_heads = n_embed // n_heads\n",
    "latent = torch.randn(batch_size, seq_len, n_embed)\n",
    "\n",
    "lat_proj = nn.Linear(n_embed, 3 * n_embed)\n",
    "k, q, v = lat_proj(latent).chunk(3, dim=-1)\n",
    "\n",
    "q_w = nn.Linear(n_embed, n_embed)\n",
    "k_w = nn.Linear(n_embed, n_embed)\n",
    "v_w = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "q = q_w(q)\n",
    "k = k_w(k)\n",
    "v = v_w(v)\n",
    "\n",
    "q = q.view(batch_size, seq_len, n_heads, d_heads).transpose(1, 2)\n",
    "k = k.view(batch_size, seq_len, n_heads, d_heads).transpose(1, 2)\n",
    "v = v.view(batch_size, seq_len, n_heads, d_heads).transpose(1, 2)\n",
    "\n",
    "weights = torch.matmul(q, k.transpose(-2, -1)) / (n_embed ** 0.5)\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "output = torch.matmul(weights, v)\n",
    "\n",
    "output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embed)\n",
    "\n",
    "print(\" # number of KV cache elements per token per layer\", n_heads * d_heads * 2)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE encoding\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, seq, n_embed=1024, theta=10000.0):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.theta = 1/(theta ** (torch.arange(0, n_embed, 2)[: n_embed //2].float() / n_embed))\n",
    "        self.seq = seq\n",
    "        self.build_rope_cache(self.seq)\n",
    "\n",
    "    def build_rope_cache(self, seq):\n",
    "\n",
    "        pos = torch.arange(0, seq, dtype=self.theta.dtype)\n",
    "        thetas = torch.einsum(\"i, j -> ij\", pos, self.theta).float()\n",
    "        rope = torch.stack([torch.cos(thetas), torch.sin(thetas)], dim=-1)\n",
    "        self.register_buffer(\"rope\", rope, persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, nhead, hdim = x.shape\n",
    "\n",
    "        x = x.view(b, s, nhead, hdim//2 , 2)\n",
    "        rope = self.rope.view(1, s, nhead, hdim//2 , 2)\n",
    "        x_out = torch.stack(\n",
    "            [\n",
    "                x[..., 0] * rope[..., 0]\n",
    "                - x[..., 1] * rope[..., 1],\n",
    "                x[..., 1] * rope[..., 0]\n",
    "                + x[..., 0] * rope[..., 1],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        x_out = x_out.flatten(3)\n",
    "        return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLatentAttn(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_c, d_r_h, d_hat_c):\n",
    "\n",
    "        super(MultiHeadLatentAttn, self).__init__()\n",
    "        assert d_model % num_heads == 0 , \"d_model should be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.d_r_h = d_r_h\n",
    "\n",
    "        # define weights \n",
    "        self.w_DQ = nn.Linear(d_model, d_hat_c) # project to smaller dimension d_hat query compression\n",
    "        self.w_UQ = nn.Linear(d_hat_c, d_model) # d_hat_c, d_model , bring the dimension back to d_model\n",
    "        self.w_QR = nn.Linear(d_hat_c, d_r_h * self.num_heads) # d_hat_c, d_model  , weights for applying RoPE encoding for queries project d_hat_c --> d_r\n",
    "        self.w_KR = nn.Linear(d_model, d_r_h) # weights for applying RoPE encoding for keys project d_model --> d_r\n",
    "\n",
    "        self.w_DKV = nn.Linear(d_model, d_c) # weight to compress keys and values into one latent\n",
    "        self.w_UK = nn.Linear(d_c, d_model)  # bring the key dimension back to d_model\n",
    "        self.w_UV = nn.Linear(d_c, d_model) # bring the value dimension back to d_model\n",
    "        self.rope_query = RoPE(seq_len, d_r_h * self.num_heads)\n",
    "        self.rope_key = RoPE(seq_len, d_r_h)\n",
    "\n",
    "\n",
    "    def forward(self, latent):\n",
    "        ## input latent shape  (batch_size, seq_len, d_model) \n",
    "        batch_size, seq_len, d = latent.shape \n",
    "\n",
    "        ct_Q = self.w_DQ(latent)    # project to smaller dimension d_hat_c query compression --> batch_size, seq_len, d_hat_c\n",
    "        qt_C = self.w_UQ(ct_Q)  # bring the dimension back to d_model -->  batch_size, seq_len, d_model\n",
    "        qt_C = qt_C.view(batch_size, seq_len, self.num_heads, self.head_dim)  # reshape to num_heads\n",
    "\n",
    "        qt_R = self.w_QR(ct_Q)  # weights for applying RoPE encoding for queries project d_hat_c --> d_r\n",
    "        qt_R = qt_R.view(batch_size, seq_len, self.num_heads, self.d_r_h)  # reshape to num_heads\n",
    "        qt_R = self.rope_query(qt_R)\n",
    "\n",
    "        qt_cat = torch.cat([qt_C, qt_R], dim=-1)  # concatenate the query with the RoPE encoding (batch_size, seq_len, num_heads, d_model//num_heads * 2)\n",
    "        qt_cat = qt_cat.transpose(1,2) # reshape to b, num_heads, seq_len, dim\n",
    "        #print(\"qt_cat\", qt_cat.shape)\n",
    "\n",
    "        ct_KV = self.w_DKV(latent)  # project to smaller dimension d_c --> batch_size, seq_len, d_c\n",
    "#\n",
    "        kt_C = self.w_UK(ct_KV)  # bring the key dimension back to d_model --> batch_size, seq_len, d_model\n",
    "        kt_C = kt_C.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)  # (batch_size, seq_len, num_heads, d_model//num_heads)\n",
    "        vt_C = self.w_UV(ct_KV)  # bring the value dimension back to d_model --> batch_size, seq_len, d_model\n",
    "        vt_C = vt_C.view(batch_size, seq_len, self.num_heads, self.head_dim)  # (batch_size, seq_len, num_heads, d_model//num_heads)\n",
    "        vt_C = vt_C.transpose(1, 2)  # reshape to b, num_heads, seq_len, dim\n",
    "        #print(\"vt_C\", vt_C.shape)\n",
    "#\n",
    "        kt_R = self.w_KR(latent)  # weights for applying RoPE encoding for keys project d_model --> d_r\n",
    "        \n",
    "        kt_R = kt_R.unsqueeze(1)   # add head dimension\n",
    "        kt_R = self.rope_key(kt_R)\n",
    "        kt_R = kt_R.expand(-1, self.num_heads, -1, -1)  # expand to num_heads\n",
    "\n",
    "        kt_cat = torch.cat([kt_C, kt_R], dim=-1)  # concatenate the key with the RoPE encoding (batch_size, seq_len, num_heads, d_model//num_heads * 2)\n",
    "        #print(\"kt_cat\", kt_cat.shape)\n",
    "        \n",
    "        weights = F.softmax(torch.matmul(qt_cat, kt_cat.transpose(-1,-2)) / ((self.d_r_h + self.head_dim)** 0.5), dim=-1)\n",
    "        #print(\"weights\", weights.shape)\n",
    "        \n",
    "        attn = torch.matmul(weights, vt_C)  # (batch_size, num_heads, seq_len, d_model//num_heads)\n",
    "\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent shape torch.Size([4, 10, 1024])\n",
      "torch.Size([4, 10, 1024])\n",
      " # number of KV cache elements per token per layer  ~ 9/2 * head_dim) 288\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "d_model = 1024  \n",
    "batch_size = 4\n",
    "seq_len = 10 \n",
    "\n",
    "# following defaults from deepseek v2 paper except d_hat_c which i set to 8\n",
    "head_dim = d_model // num_heads\n",
    "d_r_h = head_dim // 2\n",
    "d_c =  4 * head_dim\n",
    "d_hat_c = 8\n",
    "\n",
    "latent = torch.randn(batch_size, seq_len, n_embed)\n",
    "print(\"latent shape\", latent.shape)\n",
    "\n",
    "\n",
    "multi_head_attn = MultiHeadLatentAttn(\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_c=d_c,\n",
    "    d_r_h=d_r_h,\n",
    "    d_hat_c=d_hat_c\n",
    ")\n",
    "out = multi_head_attn(latent)\n",
    "#print(out.shape)\n",
    "out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embed)\n",
    "print(out.shape)\n",
    "print(\" # number of KV cache elements per token per layer  ~ 9/2 * head_dim)\", d_r_h + d_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Remarks\n",
    "\n",
    "This is an excerpt from the paper \"For DeepSeek-V2, we design an innovative attention mechanism called Multi-head Latent Attention (MLA). Equipped with low-rank key-value joint compression, MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache. We introduce its architecture in the following, and also provide a comparison between MLA and MHA in Appendix D.2.\"\n",
    "\n",
    "During inference MLA caches only $C_t^{KV}$ and $K_t^R$, so the number of elements in KV cache per token for L layers comes out to be ($d_c$ + $d_h^R$) x L\n",
    "\n",
    "![KV Cache](../../images/kv_cache_per_token.JPG)\n",
    "\n",
    "MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache. This reduces memory footprint, achieves faster inference and better hardware utilization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
