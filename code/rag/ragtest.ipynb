{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "#import getpass\n",
    "import os\n",
    "# Load environment variables from the .secrets file (or .env file)\n",
    "load_dotenv(dotenv_path='../../.secrets')\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\n",
    "#print(os.environ[\"TAVILY_API_KEY\"])\n",
    "#print(os.environ[\"LANGCHAIN_API_KEY\"])\n",
    "#print(os.environ[\"LANGCHAIN_ENDPOINT\"])\n",
    "#print(os.environ[\"ANTHROPIC_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hub.docker.com/r/ollama/ollama\n",
    "\n",
    "\n",
    "\n",
    "docker pull ollama/ollama\n",
    "\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n",
    "\n",
    "docker compose up -d\n",
    "\n",
    "docker volume create ollama-data\n",
    "docker volume create open-webui-data\n",
    "\n",
    "ollama models load llama2\n",
    "\n",
    "\n",
    "\n",
    "curl -X POST http://localhost:11434/api/generate \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\"model\": \"tinyllama\", \"prompt\": \"What are birds?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Birds are vertebrate animals that possess feathers and a fully developed beak, which allows them to feed on plant material through the process of nectar collection and seed dispersal. Birds come in all shapes and sizes, from small songbirds like robins and sparrows, to large birds such as eagles, owls, and vultures.\n",
      "\n",
      "In addition to their wings and feet, birds have feathered plumes or a beak at the end of their bills that they use to catch prey in water, on land, or in trees. Some birds also have sharp talons that help them grasp their food or protect themselves from predators. Overall, birds are highly specialized animals that require a diverse range of adaptations for their diverse environments.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class OllamaLLM(LLM, BaseModel):\n",
    "    model: str = \"tinyllama\"\n",
    "    base_url: str = \"http://localhost:11434\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[list] = None) -> str:\n",
    "       payload = {\"model\": self.model, \"prompt\": prompt}\n",
    "       try:\n",
    "           response = requests.post(\n",
    "               f\"{self.base_url}/api/generate\",\n",
    "               json=payload,\n",
    "               stream=True,  # Enable streaming\n",
    "           )\n",
    "           response.raise_for_status()\n",
    "\n",
    "           # Collect streamed responses\n",
    "           full_response = \"\"\n",
    "           for line in response.iter_lines(decode_unicode=True):\n",
    "               if line:  # Skip empty lines\n",
    "                   try:\n",
    "                       data = json.loads(line)\n",
    "                       #print(\"Streaming JSON Object:\", data)  # Debugging\n",
    "                       full_response += data.get(\"response\", \"\")\n",
    "                       if data.get(\"done\", False):\n",
    "                           break\n",
    "                   except json.JSONDecodeError as e:\n",
    "                       print(f\"Failed to decode line: {line}. Error: {e}\")\n",
    "                       continue\n",
    "\n",
    "           return full_response\n",
    "       except requests.RequestException as e:\n",
    "           raise ValueError(f\"Error communicating with Ollama API: {e}\")\n",
    "       except ValueError as e:\n",
    "           raise ValueError(f\"Error processing the response: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "ollama_llm = OllamaLLM()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What are birds?\"\n",
    "response = ollama_llm(prompt)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Response: Math is a subject that many people find difficult to understand, and for good reason. But don't worry - we're here to help! Here are some basic math terms and concepts that should make it easier to grasp the basics of mathematics:\n",
      "\n",
      "1. Math symbols: There are a few commonly used symbols in mathematics, including the letter 'x', which represents an unknown variable or part of a problem (e.g., x is missing from the equation above). Other common symbols include '=', '+', '-', '*', '/', and '(', which represents multiplication and division, respectively.\n",
      "\n",
      "2. Addition: When we add two numbers together, we're adding each number to the left of '+' (or to the right if it's negative). So for example, 4 + 8 is 12.\n",
      "\n",
      "3. Subtraction: To subtract one number from another, we're taking away each number to the left of '-' (or to the right if it's positive). So for example, 9 - 2 is 7.\n",
      "\n",
      "4. Multiplication: When we multiply two numbers together, we're multiplying each number to the right of '*' (or to the left if it's negative), and summing the products. So for example, 3 * 6 is 18.\n",
      "\n",
      "5. Division: To divide one number from another, we're dividing each number to the left of '/' (or to the right if it's a fraction), and taking away the result. So for example, 9 / 2 is 4.\n",
      "\n",
      "6. Equality: When we compare two numbers or two sets of numbers, we're comparing their values to each other. If they're equal, then we say that both are true. So for example, 3 = 3 is true.\n",
      "\n",
      "7. Fractions: Fractions represent the idea of dividing a whole number into smaller parts (e.g., 1/2). Fractions can be represented by writing 'รท' instead of using the full decimal points. So for example, 5 / 4 is 1.\n",
      "\n",
      "So there you have it! Math in simple terms.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom LLM integration for Ollama's API.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"tinyllama\"  # Default model\n",
    "    base_url: str = \"http://localhost:11434\"  # Default Ollama endpoint\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[list] = None) -> str:\n",
    "        \"\"\"\n",
    "        Handles the interaction with the Ollama API.\n",
    "        \"\"\"\n",
    "        payload = {\"model\": self.model, \"prompt\": prompt}\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                stream=True,  # Enable streaming\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Process streamed responses\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        #print(\"Streaming JSON Object:\", data)  # Debugging\n",
    "                        full_response += data.get(\"response\", \"\")\n",
    "                        if data.get(\"done\", False):  # Stop when done\n",
    "                            break\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Failed to decode line: {line}. Error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return full_response\n",
    "        except requests.RequestException as e:\n",
    "            raise ValueError(f\"Error communicating with Ollama API: {e}\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error processing the response: {e}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"Returns identifying parameters for serialization.\"\"\"\n",
    "        return {\"model\": self.model, \"base_url\": self.base_url}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Type of the LLM.\"\"\"\n",
    "        return \"ollama\"\n",
    "\n",
    "# Instantiate the Ollama LLM\n",
    "ollama_llm = OllamaLLM(model=\"tinyllama\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],  # Variables to inject\n",
    "    template=\"Explain {topic} in simple terms.\",\n",
    ")\n",
    "\n",
    "# Create an LLMChain\n",
    "#chain = LLMChain(llm=ollama_llm, prompt=prompt)\n",
    "#result = chain.run(topic=\"maths\")\n",
    "chain = prompt | ollama_llm\n",
    "# Run the chain with a specific input\n",
    "result = chain.invoke({\"topic\": \"math\"})\n",
    "print(\"LangChain Response:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
