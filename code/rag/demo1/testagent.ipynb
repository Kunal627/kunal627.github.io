{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from qdrant_client import QdrantClient\n",
    "import ollama\n",
    "\n",
    "from config import *\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client\n",
    "oclient = ollama.Client(host=Config.HOSTNAME)\n",
    "# Initialize Qdrant client\n",
    "qclient = QdrantClient(host=Config.HOSTNAME, port=Config.QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom LLM integration for Ollama's API.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"tinyllama\"  # Default model\n",
    "    base_url: str = \"http://localhost:11434\"  # Default Ollama endpoint\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[list] = None) -> str:\n",
    "        \"\"\"\n",
    "        Handles the interaction with the Ollama API.\n",
    "        \"\"\"\n",
    "        payload = {\"model\": self.model, \"prompt\": prompt}\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                stream=True,  # Enable streaming\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Process streamed responses\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        #print(\"Streaming JSON Object:\", data)  # Debugging\n",
    "                        #print(data)\n",
    "                        full_response += data.get(\"response\", \"\")\n",
    "                        if data.get(\"done\", False):  # Stop when done\n",
    "                            break\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Failed to decode line: {line}. Error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return full_response\n",
    "        except requests.RequestException as e:\n",
    "            raise ValueError(f\"Error communicating with Ollama API: {e}\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error processing the response: {e}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"Returns identifying parameters for serialization.\"\"\"\n",
    "        return {\"model\": self.model, \"base_url\": self.base_url}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Type of the LLM.\"\"\"\n",
    "        return \"ollama\"\n",
    "\n",
    "# Instantiate the Ollama LLM\n",
    "ollama_llm = OllamaLLM(model=\"tinyllama\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> localhost 11434\n",
      ">>>>>>> localhost 6333\n",
      "Context given to tinyllama >>>>>>\n",
      "ic systems lies in enabling intuitive control and seamless human-robot interaction when the user's input capabilities are severely limited. many end-users, such as those with impaired motor function or residual muscular control, may have access to only low degrees of freedom (dof) or low-bandwidth control interfaces. however, sought-after robotic assistance tasks such as dexterous object manipulation often requires high-dimensional control in a three-dimensional space. mapping these low-dof user inputs, such as facial gestures, breath control, or limited extremity movements, onto the full 6-dof control space for robotic manipulation presents a highly non-trivial mapping challenge. current high-dof robotic manipulators generally lack the intelligence and flexibility to adaptively address this assistance level tradeoff through appropriate boosting algorithms that can satisfy users with limited motor function. in such complex interactive dynamics, an assistive artificial intelligence (ai) agent is crucial for not only generating the remaining motor function required to complete high-dimensional manipulation tasks but also addressing the desired assistance level tradeoff. the developed shared embodied machine intelligence should understand, boost, and shape the end-users limited function, enabling collaborative completion of dexterous manipulation tasks in an intuitive, natural manner that meets daily needs. to address this gap, we have developed an ai-based algorithm that utilizes computer vision techniques to create a solid framework for human-robot teaming even with low-dof user input. in our preliminary results, we have successfully amplified a 1-dof user input to a 3-dof manipulation task. figure 1 shows an example of our algorithm implemented on a kinova jaco2 assistive robotic arm, where the human provides input across the y-axis (left or right), and the ai agent amplifies this input to all three axes, enabling the robot to complete pick-and-place tasks. figure 1: a successful attempt for a pick-and-place task 3With recent advancements in AI and computation tools, intelligent paradigms emerged to empower different fields such as healthcare robots with new capabilities. Advanced AI robotic algorithms (e.g., reinforcement learning) can be trained and developed to autonomously make individual decisions to achieve a desired and usually fixed goal. However, such independent decisions and goal achievements might not be ideal for a healthcare robot that usually interacts with a dynamic end-user or a patient. In such a complex human-robot interaction (teaming) framework, the dynamic user continuously wants to be involved in decision-making as well as introducing new goals while interacting with their present environment in real-time. To address this challenge, an adaptive shared autonomy AI paradigm is required to be developed for the two interactive agents (Human & AI agents) with a foundation based on human-centered factors to avoid any possible ethical issues and guarantee no harm to humanity. 1 toward human-centered shared autonomy ai paradigms for human-robot teaming in healthcare reza abiri, ali rabiee, sima ghafoori, anna cetera translational neurorobotics laboratory ( dept. of electrical, computer and biomedical engineering university of rhode island kingston, ri, usa contact information: reza_abiri@uri.edu all authors contributed equally to this white paper report. 1. abstract with recent advancements in ai and computation tools, intelligent paradigms emerged to empower different fields such as healthcare robots with new capabilities. advanced ai robotic algorithms (e.g., reinforcement learning) can be trained and developed to autonomously make individual decisions to achieve a desired and usually fixed goal. however, such independent decisions and goal achievements might not be ideal for a healthcare robot that usually interacts with a dynamic end-user or a patient. in such a complex human-robot interaction (teaming) framework, the dynamic user continuously wants to be involved in decision-making as well as introducing new goals while interacting with their present environment in real-time. to address this challenge, an adaptive shared autonomy ai paradigm is required to be developed for the two interactive agents (human & ai agents) with a foundation based on human- centered factors to avoid any possible ethical issues and guarantee no harm to humanity. 2. robotics in healthcare and human-robot interaction the application of the robotics field in healthcare has expanded during the past years, particularly with the spread of covid-19. as a remotely accessible tool to patients, nursing robots  and surgical robots  have been studied by different scientific groups. these robots are usually required to be controlled by an operator or a surgeon through a shared control paradigm. brain-controlled assistive robots, an emerging category in healthcare robotics, have been introduced to facilitate a close physical human-robot interaction framework to restore the physical movements of severely paralyzed patients for independent living . additionally, rehabilitation robots have been used with direct contact with patients (e.g., strokes) to improve motor performance by recovery of cortical plasticity . social robots were designed to assist elderly patients with cognitive tasks . 3. traditional shared control paradigms developing a blended, intelligent, and balanced framework of human-robot teamwork has become an essential element, particularly in healthcare robots where direct human (operator, patient) engagement plays an important role in overall reported performance satisfaction . to this end, different shared control paradigms were developed to combine human and robot inputs through sequential , parallel, or blended structures . in most of these studies, traditional and simple control and machine learning algorithms were used to achieve a mutual goal. for example, noninvasive joystick movements  or peripheral signals  were used as a portion of inputs alongside human input into a shared controlled assistive robotic arm platform to complete reach tasks with an inverse kinematics model. invasive brain- controlled approaches  were employed to share the control for a reach task using traditional machine learning techniques. a limited number of studies have targeted the application of shared control theories in restoring reach tasks in stroke patients using robotic exoskeletons . 4. innovative shared autonomy paradigms with advancements in computational tools over the past decade, the field of robotics has also been influenced by learning algorithms such as the game theory algorithm  and reinforcement learning (rl) . particularly, complex rl with deep learning layer mechanisms (deep rl) has become popular in learning the representation of an object for autonomous manipulation , performing dexterous 2 manipulation tasks [29, 32-36], semantics learning of grasp  and end-to-end visuomotor control  using robotic arms. leveraging these algorithms for modeling the shared autonomy dynamics in human- robot interaction introduces the possibility of a more intuitive interaction with healthcare robots. in this shared autonomy paradigm , rather than using shared control strategies such as sequential inputs generated by the human and robot  or a simple summation of those inputs , an ai algorithm (deep rl) mechanism can provide an approach/method? to blend the human and robotic inputs for advancing adaptive and intuitive control. 5. human-centered ai for shared autonomy paradigms in recent years, the concept of human-ai collaborative algorithms was revised to human-centered ai (hcai) algorithms for human-ai teaming  in different fields. human, technology, and ethics are the three main factors in hcai paradigms . each factor is essential for the design of an effective human- ai team. for example, if the final product of a human-ai robotic system only considers ai technology and ethics but ignores human factors (e.g., authority, needs, user experience), the system may become unusable or even harmful to the user . as reported in previous studies, fatigue and cognitive load experienced by the users were parts of translational barriers in brain-controlled high-dof robotic arms  human-centered ai factors is essential and will be prominent in shared autonomy paradigms where two agents (human, ai agent) interact and collaborate to achieve the human’s desired goals. ai agents should always be supervised by the user to guarantee ethical regulation and safe experiences. 6. a case study a critical challenge in developing assistive robotic systems lies in enabling intuitive control and seamless human-robot interaction when the user's input capabilities are severely limited. many end-users, such as those with impaired motor function or residual muscular control, may have access to only low degrees of freedom (dof) or low-bandwidth control interfaces. however, sought-after robotic assistance tasks such as dexterous object manipulation often requires high-dimensional control in a three-dimensional space. mapping these low-dof user inputs, such as facial gestures, breath control, or limited extremity movements, onto the full 6-dof control space for robotic manipulation presents a highly non-trivial mapping challenge. current high-dof robotic manipulators generally lack the intelligence and flexibility to adaptively address this assistance level tradeoff through appropriate boosting algorithms that can satisfy users with limited motor function. in such complex interactive dynamics, an assistive artificial intelligence (ai) agent is crucial for not only generating the remaining motor function required to complete high-dimensional manipulation tasks but also addressing the desired assistance level tradeoff. the developed shared embodied machine intelligence should understand, boost, and shape the end-users limited function, enabling collaborative completion of dexterous manipulation tasks in an intuitive, natural manner that meets daily needs. to address this gap, we have developed an ai-based algorithm that utilizes computer vision techniques to create a solid framework for human-robot teaming even with low-dof user input. in our preliminary results, we have successfully amplified a 1-dof user input to a 3-dof manipulation task. figure 1 shows an\n",
      "LangChain Response: To address the challenge of developing assistive robotic systems for end-users with limited motor function or residual muscular control, cognitive science can help design and implement an ai-based algorithm that allows for intuitive control in a high-dimensional space. By utilizing computer vision techniques to create a solid framework for human-robot teaming, the system can generate the remaining motor function required to complete high-dimensiional manipulation tasks while still addressing the desired assistance level tradeoff for end-users with limited input capabilities. This will ensure that assistive ai agents are capable of providing intuitive and effective assistance in situations where traditional robotic solutions may not be feasible or appropriate. Overall, by leveraging cognitive science principles, we can develop a more intuitive and adaptable ai-based approach for enabling collaborative interactions between human and machine in complex interactive scenarios.\n"
     ]
    }
   ],
   "source": [
    "ask_query = \" Can you tell me more on shared autonomy AI paradigms for human-robot teaming?\"\n",
    "model = \"tinyllama\"\n",
    "query_embed , _ = get_embeddings(model, [ask_query], llm_client=oclient)\n",
    "query_embed = query_embed[0]\n",
    "\n",
    "hits = retrieve_context(query_embed=query_embed, vdb_client=qclient)\n",
    "context = \"\"\n",
    "for hit in hits:\n",
    "    #print(hit.payload, \"score:\", hit.score)\n",
    "    context += hit.payload['text']\n",
    "\n",
    "\n",
    "print(\"Context given to tinyllama >>>>>>\")\n",
    "context = context.replace(\"\\n\", \" \")\n",
    "print(context)\n",
    "\n",
    "\n",
    "# Create a PromptTemplate\n",
    "topic = \"autonomy aii paradigms for human robot teaming\"\n",
    "#query = quert_text\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant. You will be given a context and a question. Please answer the question based on the context provided. Ensure your answer is clear, concise, and relevant to the information in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant. Could you please answer a question {query} based on the context {context}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],  # Variables to inject\n",
    "    template=prompt,\n",
    ")\n",
    "\n",
    "# chain prompt and llm\n",
    "chain = prompt_template | ollama_llm\n",
    "# Run the chain with a specific input\n",
    "result = chain.invoke({'query': ask_query, 'context': context})\n",
    "print(\"LangChain Response:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_query = \"Who is the author of the paper Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration\"\n",
    "topic = \"Accounting for Human Beliefs\"\n",
    "\n",
    "new_query += topic\n",
    "\n",
    "\n",
    "new_embed, _ = get_embeddings(model, [new_query], llm_client=oclient)\n",
    "new_embed = new_embed[0]\n",
    "\n",
    "hits = retrieve_context(new_embed, vdb_client=qclient)\n",
    "new_context = \"\"\n",
    "for hit in hits:\n",
    "    new_context += hit.payload['text']\n",
    "\n",
    "print(new_context)\n",
    "\n",
    "result = chain.invoke({'context': new_context, 'topic': topic, 'query': new_query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and print the result\n",
    "#Config.COLLECTION_NAME = \"arvix_papers\"\n",
    "#collections = qclient.get_collections()\n",
    "#existing_coll = [collection.name for collection in collections.collections]\n",
    "#print(existing_coll)\n",
    "#vector_count = get_vector_count(Config.COLLECTION_NAME, vdb_client=qclient)\n",
    "#print(f\"Number of vectors in the collection '{Config.COLLECTION_NAME}': {vector_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
