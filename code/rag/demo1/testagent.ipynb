{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from qdrant_client import QdrantClient\n",
    "import ollama\n",
    "from config import *\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client\n",
    "oclient = ollama.Client(host=Config.HOSTNAME)\n",
    "# Initialize Qdrant client\n",
    "qclient = QdrantClient(host=Config.HOSTNAME, port=Config.QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom LLM integration for Ollama's API.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"tinyllama\"  # Default model\n",
    "    base_url: str = \"http://localhost:11434\"  # Default Ollama endpoint\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[list] = None) -> str:\n",
    "        \"\"\"\n",
    "        Handles the interaction with the Ollama API.\n",
    "        \"\"\"\n",
    "        payload = {\"model\": self.model, \"prompt\": prompt}\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                stream=True,  # Enable streaming\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Process streamed responses\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        #print(\"Streaming JSON Object:\", data)  # Debugging\n",
    "                        #print(data)\n",
    "                        full_response += data.get(\"response\", \"\")\n",
    "                        if data.get(\"done\", False):  # Stop when done\n",
    "                            break\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Failed to decode line: {line}. Error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return full_response\n",
    "        except requests.RequestException as e:\n",
    "            raise ValueError(f\"Error communicating with Ollama API: {e}\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error processing the response: {e}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"Returns identifying parameters for serialization.\"\"\"\n",
    "        return {\"model\": self.model, \"base_url\": self.base_url}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Type of the LLM.\"\"\"\n",
    "        return \"ollama\"\n",
    "\n",
    "# Instantiate the Ollama LLM\n",
    "ollama_llm = OllamaLLM(model=\"tinyllama\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> localhost 11434\n",
      ">>>>>>> localhost 6333\n",
      "LangChain Response: Sure! Here's a succinct version of my answer to your question:\n",
      "\n",
      "Yes, I can provide you with some information about distributed quantum computing. Distributed quantum computing refers to the use of quantum mechanics for performing complex calculations on distributed nodes over an Internet-scale network. This involves using quantum computers to solve problems that are computationally intensive but impossible to solve using classical computers. Examples of such problems include cryptography, signal processing, and drug discovery. In terms of applications, distributed quantum computing has the potential to revolutionize various industries like finance, healthcare, and manufacturing, as it can perform highly complex calculations in real-time, without any need for storage or centralized control. Overall, distributed quantum computing offers significant advancements in fields like cryptography and drug discovery, as well as many other potential applications that will shape our future technological landscape.\n"
     ]
    }
   ],
   "source": [
    "ask_query = \" Can you tell me something about distributed quantum computing\"\n",
    "model = \"tinyllama\"\n",
    "query_embed , _ = get_embeddings(model, [ask_query], llm_client=oclient)\n",
    "query_embed = query_embed[0]\n",
    "\n",
    "hits = retrieve_context(query_embed=query_embed, vdb_client=qclient)\n",
    "context = \"\"\n",
    "for hit in hits:\n",
    "    #print(hit.payload, \"score:\", hit.score)\n",
    "    context += hit.payload['text']\n",
    "\n",
    "#print(context)\n",
    "\n",
    "\n",
    "# Create a PromptTemplate\n",
    "topic = \"quantum computing\"\n",
    "#query = quert_text\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an assistant who answers questions based on provided context and a specific topic.\n",
    "Keep the response succinct under 50 words. Exclude the context and query in final answer \n",
    "context and query text in the answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Topic:\n",
    "{topic}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"topic\", \"query\"],  # Variables to inject\n",
    "    template=prompt,\n",
    ")\n",
    "\n",
    "# chain prompt and llm\n",
    "chain = prompt_template | ollama_llm\n",
    "# Run the chain with a specific input\n",
    "result = chain.invoke({'context': context, 'topic': topic, 'query': ask_query})\n",
    "print(\"LangChain Response:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> localhost 11434\n",
      ">>>>>>> localhost 6333\n",
      "The answer to the question \"On which date the paper on rise of quantum computing was published\" is as follows: The paper titled \"The Risese of Quantum Internet Computing\" was published in the August issue of IEEE IoT Magazine, Vol. XX, No. X, X 2022.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_query = \"On which date the paper on rise of quantum computing published and in which publication?\"\n",
    "topic = \"quantum computing\"\n",
    "\n",
    "new_embed, _ = get_embeddings(model, [new_query], llm_client=oclient)\n",
    "new_embed = new_embed[0]\n",
    "\n",
    "hits = retrieve_context(new_embed, vdb_client=qclient)\n",
    "new_context = \"\"\n",
    "for hit in hits:\n",
    "    new_context += hit.payload['text']\n",
    "\n",
    "#print(new_context)\n",
    "\n",
    "result = chain.invoke({'context': new_context, 'topic': topic, 'query': new_query})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
