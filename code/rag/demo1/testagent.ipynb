{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from qdrant_client import QdrantClient\n",
    "import ollama\n",
    "\n",
    "from config import *\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client\n",
    "oclient = ollama.Client(host=Config.HOSTNAME)\n",
    "# Initialize Qdrant client\n",
    "qclient = QdrantClient(host=Config.HOSTNAME, port=Config.QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom LLM integration for Ollama's API.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"tinyllama\"  # Default model\n",
    "    base_url: str = \"http://localhost:11434\"  # Default Ollama endpoint\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[list] = None) -> str:\n",
    "        \"\"\"\n",
    "        Handles the interaction with the Ollama API.\n",
    "        \"\"\"\n",
    "        payload = {\"model\": self.model, \"prompt\": prompt}\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                stream=True,  # Enable streaming\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Process streamed responses\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        #print(\"Streaming JSON Object:\", data)  # Debugging\n",
    "                        #print(data)\n",
    "                        full_response += data.get(\"response\", \"\")\n",
    "                        if data.get(\"done\", False):  # Stop when done\n",
    "                            break\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Failed to decode line: {line}. Error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return full_response\n",
    "        except requests.RequestException as e:\n",
    "            raise ValueError(f\"Error communicating with Ollama API: {e}\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error processing the response: {e}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"Returns identifying parameters for serialization.\"\"\"\n",
    "        return {\"model\": self.model, \"base_url\": self.base_url}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Type of the LLM.\"\"\"\n",
    "        return \"ollama\"\n",
    "\n",
    "# Instantiate the Ollama LLM\n",
    "ollama_llm = OllamaLLM(model=\"tinyllama\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> localhost 11434\n",
      "query collection test_rag_agent\n",
      ">>>>>>> localhost 6333\n",
      "Context given to tinyllama >>>>>>\n",
      "zeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., Polosukhin,  I. (2017). Attention is all you need. Advances in neural information processing systems, 30.  http://arxiv.org/abs/1706.03762  Wei, J., Kim, S., Jung, H., Kim, Y. (2023). Leveraging Large Language Models to Power Chatbots  for Collecting User Self-Reported Data. arXiv:2301.05843v1 [cs.HC].  https://doi.org/10.48550/arXiv.2301.05843  White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A.eceived positive ratings (“good” or “excellent”) for overall performance, with similar distributions for question quality (96%) and advice appropriateness (94%). No conversation was deemed potentially dangerous overall. In the assessment of medical accuracy, 95% of conversations contained no inaccuracies, with one conversation flagged for the presence of potentially dangerous inaccuracies. 5% 95% 6% 81% 12% 4% 84% 12% 5% 84% 12% * Accuracy (n=298) Advice (n=298) Questions (n=298) Overall (n=298)–489, December 2023. URL: http://dx.doi.org/10.1016/j.mcpdig.2023.08.002, doi: 10.1016/j.mcpdig.2023.08.002. [12] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster,With recent advancements in AI and computation tools, intelligent paradigms emerged to empower different fields such as healthcare robots with new capabilities. Advanced AI robotic algorithms (e.g., reinforcement learning) can be trained and developed to autonomously make individual decisions to achieve a desired and usually fixed goal. However, such independent decisions and goal achievements might not be ideal for a healthcare robot that usually interacts with a dynamic end-user or a patient. - fram Burgard. 2023a. Visual language maps for robot navigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 10608– 10615. IEEE. 8 Qiuyuan Huang, Jae Sung Park, Abhinav Gupta, Paul Bennett, Ran Gong, Subhojit Som, Baolin Peng, Owais Khan Mohammed, Chris Pal, Yejin Choi, et al. 2023b. Ark: Augmented reality with knowl- edge interactive emergent ability. arXiv preprint arXiv:2305.00970. 7, 8, 21, 22 Qiuyuan Huang, Pengchuan Zhang, Oliver Wu, and Lei Zhang. 2018.ugh ChatGPT’s built-in features.  A patient’s case from the American College of Radiology has been selected as an example to  compare the outputs of generic ChatGPT with the specialized chatbot as seen in Figure 2 (Rao et al.,  2023). Both outputs use zero-shot prompting; in other words, the LLM is given no examples to learn  from before engaging in dialogue from the user. The output on the left is by generic ChatGPT while  the output on the right is by the specialized chatbot avatar.   Figure 2n our  preliminary results, we have successfully amplified a 1-DOF user input to a 3-DOF manipulation task. Figure  1 shows an example of our algorithm implemented on a Kinova Jaco2 assistive robotic arm, where the  human provides input across the y-axis (left or right), and the AI agent amplifies this input to all three axes,  enabling the robot to complete pick-and-place tasks.       Figure 1: A successful attempt for a pick-and-place task    3    References  [1]  N. Maalouf, A. Sidaoui, I. H.an, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. 5, 7 Yang Ye, Hengxu You, and Jing Du. 2023. Im- proved trust in human-robot collaboration with chat- gpt. IEEE Access. 8 Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- enmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic in- ference over event descriptions. Proceedings of the Annual Meeting of the Association for Computational Linguistics. 9In such a complex human-robot interaction (teaming) framework, the dynamic user continuously wants to be involved in decision-making as well as introducing new goals while interacting with their present environment in real-time. To address this challenge, an adaptive shared autonomy AI paradigm is required to be developed for the two interactive agents (Human & AI agents) with a foundation based on human-centered factors to avoid any possible ethical issues and guarantee no harm to humanity.  1 s, and show promising results of how the interactive agent sys- tem can further boost the large foundation models in our setting. It integrates and improves the depth of generalization, conscious and interpretability of a complex adaptive AI systems. 9 Impact Statement Agent AI paradigm is to create general-purpose agents that can work alongside humans in both real and virtual environments. This paradigm therefore intends to have a very broad impact, possibly affect- ing all members of society. \n",
      "LangChain Response:  The article provides an overview of the challenges and opportunities associated with developing a shared autonomous AI paradigm for human-robot teaming. This approach involves integrating and improving the depth of generalization, consciousness, and interpretability of complex adaptive AI systems that can work alongside humans in both real and virtual environments. \n",
      "\n",
      "The challenge is to balance the need for effective decision making on the part of the robot with the human's desire for involvement in the decision-making process. This requires a foundation based on human-centered factors, such as avoiding any potential harm to humanity, ensuring no harm to the environment, and maintaining trust between humans and robots.\n",
      "\n",
      "The article suggests that an adaptive shared autonomy AI paradigm can be developed by integrating various elements, including reinforcement learning, visual language maps, and cognitive computing. These technologies aim to provide a more dynamic and responsive system for human-robot interaction, resulting in improved performance, trust, and overall efficiency.\n",
      "\n",
      "The article concludes that the shared autonomous AI paradigm for human-robot teaming has significant potential benefits for society, including improved decision-making capabilities and increased access to advanced technology. However, it also raises challenges related to safety and security concerns, as well as ensuring that the system remains aligned with human values and goals.\n"
     ]
    }
   ],
   "source": [
    "ask_query = \" Can you tell me more on shared autonomy AI paradigms for human-robot teaming?\"\n",
    "model = \"tinyllama\"\n",
    "query_embed , _ = get_embeddings(model, [ask_query], llm_client=oclient)\n",
    "query_embed = query_embed[0]\n",
    "\n",
    "hits = retrieve_context(query_embed=query_embed, vdb_client=qclient)\n",
    "context = \"\"\n",
    "for hit in hits:\n",
    "    #print(hit.payload, \"score:\", hit.score)\n",
    "    context += hit.payload['text']\n",
    "\n",
    "\n",
    "print(\"Context given to tinyllama >>>>>>\")\n",
    "context = context.replace(\"\\n\", \" \")\n",
    "print(context)\n",
    "\n",
    "\n",
    "# Create a PromptTemplate\n",
    "topic = \"AI agents in healthcare\"\n",
    "#query = quert_text\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant. You will be given a context and a question. Please answer the question based on the context provided. Ensure your answer is clear, concise, and relevant to the information in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"topic\", \"query\"],  # Variables to inject\n",
    "    template=prompt,\n",
    ")\n",
    "\n",
    "# chain prompt and llm\n",
    "chain = prompt_template | ollama_llm\n",
    "# Run the chain with a specific input\n",
    "result = chain.invoke({'context': context, 'topic': topic, 'query': ask_query})\n",
    "print(\"LangChain Response:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_query = \"Who is the author of the paper Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration\"\n",
    "topic = \"Accounting for Human Beliefs\"\n",
    "\n",
    "new_query += topic\n",
    "\n",
    "\n",
    "new_embed, _ = get_embeddings(model, [new_query], llm_client=oclient)\n",
    "new_embed = new_embed[0]\n",
    "\n",
    "hits = retrieve_context(new_embed, vdb_client=qclient)\n",
    "new_context = \"\"\n",
    "for hit in hits:\n",
    "    new_context += hit.payload['text']\n",
    "\n",
    "print(new_context)\n",
    "\n",
    "result = chain.invoke({'context': new_context, 'topic': topic, 'query': new_query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and print the result\n",
    "Config.COLLECTION_NAME = \"arvix_papers\"\n",
    "collections = qclient.get_collections()\n",
    "existing_coll = [collection.name for collection in collections.collections]\n",
    "print(existing_coll)\n",
    "vector_count = get_vector_count(Config.COLLECTION_NAME, vdb_client=qclient)\n",
    "print(f\"Number of vectors in the collection '{Config.COLLECTION_NAME}': {vector_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
