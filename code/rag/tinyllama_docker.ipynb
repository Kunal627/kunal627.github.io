{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started with LangChain: Creating a Simple Prompt and Model Chain\n",
    "\n",
    "LangChain makes building applications powered by language models seamless and efficient. One of its core features is the ability to create prompt-and-model chains, enabling you to link structured input prompts with specific models to generate meaningful outputs. In this post, we’ll walk through creating a simple LangChain prompt and model chain, perfect for beginners looking to understand the basics of this powerful framework. Let’s dive in!\n",
    "\n",
    "I picked up Llama models because, from a pay-for-API perspective, they eliminate the recurring costs associated with proprietary APIs like OpenAI and Anthropic. I picked up Tiny Llama because of compute limitations. Nevertheless, this should not stop me from using it to build intuition for learning, experimenting, and understanding the fundamentals of working with language models. By integrating it with LangChain's RAG (Retrieval-Augmented Generation) agents, I can test and explore advanced workflows, like combining knowledge retrieval with lightweight models, even in resource-constrained environments.\n",
    "\n",
    "\n",
    "#### Create Custom LLM wrapper\n",
    "\n",
    "LangChain provides integraton to llama by [Llamafile](https://python.langchain.com/docs/integrations/llms/llamafile/). Which i would take up in separate post. I have created a custom wrapper for tinyllama for this example. The instructions to writing custom implementation are [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/custom_llm/)\n",
    "\n",
    "The __call method takes in prompt and makes an API call to the ollama endpoint (hosted on docker). \n",
    "And the small for loop is to iterate over the response line by lline and stop when tiny llama is done generating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Optional\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Response: Physics is the study of how matter and energy behave at all scales, from subatomic particles to galaxies, and encompasses many different branches of science. Here are some key concepts and terminology that you might encounter:\n",
      "\n",
      "1. Matter (or substances): The physical components of objects such as atoms, molecules, or gases.\n",
      "\n",
      "2. Energy: A fundamental aspect of matter, energy is the ability to do work or move something. It can come in different forms such as kinetic, potential, and thermal.\n",
      "\n",
      "3. Force: This is a force that acts over short distances between two objects (or two objects acting on one another). Force is measured by mass times acceleration. For example, gravity causes objects to pull on each other.\n",
      "\n",
      "4. Motion: The state of being in motion, whether in space or time.\n",
      "\n",
      "5. Pressure: An amount of force applied per unit area. Pressure can occur when two objects are held apart, or when fluids flow through spaces.\n",
      "\n",
      "6. Force-momentum diagram: A graph showing the relationship between mass and acceleration, based on Newton's laws of motion. The line represents constant acceleration, so the points along it represent states of rest (when the object is at a constant velocity). \n",
      "\n",
      "7. Kinetic energy: This is the potential energy that exists when an object is in motion, relative to its rest state.\n",
      "\n",
      "8. Temperature: The average amount of energy available per unit mass, measured in kelvins. A high temperature means more energy, while a low temperature means less energy.\n",
      "\n",
      "9. Electromagnetism: This branch of physics involves the study of how electricity and magnetism interact with each other. It can be divided into two main branches: electrical and magnetic fields, and electromagnetic waves (radio waves, X-rays, etc.).\n",
      "\n",
      "10. Sound: This is a wave in the air, made up of energy carried by sound vibrations. Sounds are composed of a series of vibrations that can be heard at different frequencies.\n",
      "\n",
      "These are just some of the basic concepts and terminology used to study physics. The more you understand physics, the more amazing things will become clear to you!\n"
     ]
    }
   ],
   "source": [
    "class OllamaLLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom LLM integration for Ollama's API.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"tinyllama\"  # Default model\n",
    "    base_url: str = \"http://localhost:11434\"  # Default Ollama endpoint\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[list] = None) -> str:\n",
    "        \"\"\"\n",
    "        Handles the interaction with the Ollama API.\n",
    "        \"\"\"\n",
    "        payload = {\"model\": self.model, \"prompt\": prompt}\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json=payload,\n",
    "                stream=True,  # Enable streaming\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Process streamed responses\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        #print(\"Streaming JSON Object:\", data)  # Debugging\n",
    "                        full_response += data.get(\"response\", \"\")\n",
    "                        if data.get(\"done\", False):  # Stop when done\n",
    "                            break\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Failed to decode line: {line}. Error: {e}\")\n",
    "                        continue\n",
    "\n",
    "            return full_response\n",
    "        except requests.RequestException as e:\n",
    "            raise ValueError(f\"Error communicating with Ollama API: {e}\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error processing the response: {e}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"Returns identifying parameters for serialization.\"\"\"\n",
    "        return {\"model\": self.model, \"base_url\": self.base_url}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Type of the LLM.\"\"\"\n",
    "        return \"ollama\"\n",
    "\n",
    "# Instantiate the Ollama LLM\n",
    "ollama_llm = OllamaLLM(model=\"tinyllama\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],  # Variables to inject\n",
    "    template=\"Explain {topic} in simple terms.\",\n",
    ")\n",
    "\n",
    "# chain prompt and llm\n",
    "chain = prompt | ollama_llm\n",
    "# Run the chain with a specific input\n",
    "result = chain.invoke({\"topic\": \"physics\"})\n",
    "print(\"LangChain Response:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
