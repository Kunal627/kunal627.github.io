{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Agents with LangChain and LLMs locally\n",
    "\n",
    "Welcome to a series of blog posts where we explore **LangChain** and **RAG (Retrieval-Augmented Generation)** agents. In this series, you’ll learn how to set up a local LLM (Large Language Model), use LangChain for effective prompt management, and build powerful RAG agents for natural language processing tasks.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to LangChain and RAG](#introduction-to-langchain-and-rag)\n",
    "2. [Setting Up a Local LLM](#setting-up-a-local-llm)\n",
    "3. [Simple Prompt and model chain](#simple-prompt-and-model-chain)\n",
    "4. [Building a Basic RAG Agent](#building-a-basic-rag-agent)\n",
    "4. [Integrating LangChain with LLMs](#integrating-langchain-with-llms)\n",
    "5. [Advanced RAG Agent Features](#advanced-rag-agent-features)\n",
    "6. [Final Project: Complete RAG Agent](#final-project-complete-rag-agent)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to LangChain and RAG\n",
    "\n",
    "I will not write a post on this as there are lots of resources available on web. [langchain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up a Local LLM\n",
    "\n",
    "In this post, we'll walk through the process of setting up a local LLM using Docker and using it for LangChain-based applications. We’ll use **Ollama** for this demonstration, but the steps are similar for other LLMs.\n",
    "\n",
    "### Key Steps:\n",
    "- Installing Docker and setting up the LLM container.\n",
    "- Interacting with LLM via API.\n",
    "- Testing your local LLM for basic text generation tasks.\n",
    "\n",
    "**Python file**: [llamma_setup](https://github.com/Kunal627/kunal627.github.io/blob/main/code/rag/llamma_setup.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Prompt and model chain\n",
    "\n",
    "Learn how to create a custom LLM wrapper for TinyLlama using LangChain, build a prompt and LLM chain, and run it locally\n",
    "\n",
    "### Key Steps:\n",
    "- Custom LLM wrapper implementation\n",
    "- prompt the tinyllama model and get a response\n",
    "\n",
    "**Python file**: [llamma_setup](https://github.com/Kunal627/kunal627.github.io/blob/main/code/rag/tinyllama_docker.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
