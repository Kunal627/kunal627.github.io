{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Agents with LangChain and LLMs locally\n",
    "\n",
    "Welcome to a series of blog posts where we explore **LangChain** and **RAG (Retrieval-Augmented Generation)** agents. In this series, you’ll learn how to set up a local LLM (Large Language Model), use LangChain for effective prompt management, and build powerful RAG agents for natural language processing tasks.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to LangChain and RAG](#introduction-to-langchain-and-rag)\n",
    "2. [Setting Up a Local LLM](#setting-up-a-local-llm)\n",
    "3. [Building a Basic RAG Agent](#building-a-basic-rag-agent)\n",
    "4. [Integrating LangChain with LLMs](#integrating-langchain-with-llms)\n",
    "5. [Advanced RAG Agent Features](#advanced-rag-agent-features)\n",
    "6. [Final Project: Complete RAG Agent](#final-project-complete-rag-agent)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to LangChain and RAG\n",
    "\n",
    "I will not write a post on this as there are lots of resources available on web. [langchain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "---\n",
    "\n",
    "## Setting Up a Local LLM\n",
    "\n",
    "In this post, we'll walk through the process of setting up a local LLM using Docker and using it for LangChain-based applications. We’ll use **Ollama** for this demonstration, but the steps are similar for other LLMs.\n",
    "\n",
    "### Key Steps:\n",
    "- Installing Docker and setting up the LLM container.\n",
    "- Interacting with LLM via API.\n",
    "- Testing your local LLM for basic text generation tasks.\n",
    "\n",
    "**Python file**: [llamma_setup](https://github.com/Kunal627/kunal627.github.io/blob/main/code/rag/llamma_setup.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
