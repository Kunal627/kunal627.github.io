{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Llama 3.1 8b Locally on Windows with LlamaIndex: A Practical Guide\n",
    "\n",
    "In this blog post, we'll explore how to run LlamaIndex with the Llama 3.1 8B parameters locally on a Windows machine. LlamaIndex is a powerful tool for integrating large language models with external data sources, and we will demonstrate how to leverage a PDF document and a vectorstore for efficient information retrieval. Whether you're new to LlamaIndex or looking for a practical guide to running Llama locally, this post will walk you through the steps to set up and harness the capabilities of these advanced models for your own projects.\n",
    "\n",
    "\n",
    "#### Setting up LLama 3.1 8b model locally on Windows\n",
    "\n",
    "1. [Download](https://ollama.com/download/windows) installable for Windows\n",
    "2. [Search](https://ollama.com/search) llama 3.1 and copy the command from model page\n",
    "3. Run the command on Windows cli\n",
    "    ollama run llama3.1\n",
    "\n",
    "4. Once up and running \n",
    "\n",
    "![Llama 3.1](./llama3.1.PNG)\n",
    "\n",
    "5. Or, hit http://localhost:11434/\n",
    "\n",
    "    The browser shows \"Ollama is running\"\n",
    "\n",
    "\n",
    "NOTE: I am running this on i9 Intel vPRO with 64.0 GB with NVIDIA RTX 3500 ADA (12 GB dedicated RAM). You can try tiny llama in case of resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from common.common import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf preprocessing\n",
    "\n",
    "#### Get the sample pages from Pdf\n",
    "\n",
    "I have taken Learning Spark 2.0 pdf and extracted only one chapter for this post. The function ``` get_page_range ``` extracts a page range and save it to a output pdf. In this case i have already created ``` sample.pdf ```. use the following code snippet to get the sample pdf.\n",
    "\n",
    "```python\n",
    "text = get_page_range(\"./LearningSpark2.0.pdf\", \"sample.pdf\", 24, 41)\n",
    "```\n",
    "\n",
    "#### Extract text from pdf\n",
    "\n",
    "I have written small parsing logic to keep the text for a section together to preserve the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = fitz.open(\"./sample.pdf\")\n",
    "parsed_content = parse_pdf_sections(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Chunking the content\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for main_section_key, main_section_value in parsed_content.items():\n",
    "    for section_key, section_value in main_section_value.items():\n",
    "        for sub_section_key, sub_section_value in section_value.items():\n",
    "            metadata = (main_section_key + \" \" + section_key + \" \" + sub_section_key).strip().split()\n",
    "            chnk = fixed_size_chunking(sub_section_value, metadata, 1000, 200, char=False)\n",
    "            txt_chnk = [' '.join(c) for c in chnk]\n",
    "            chunks.extend(txt_chnk)\n",
    "            lowercased_list = [str(item).lower() for item in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# Settings\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "index = VectorStoreIndex([])\n",
    "for chunk in lowercased_list:\n",
    "    index.insert(Document(text=chunk, extra_info={}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Spark SQL\n",
      "• Spark MLlib\n",
      "• Spark Structured Streaming\n",
      "• GraphX\n"
     ]
    }
   ],
   "source": [
    "# Use locally running Ollama Server for querying the index\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model = \"llama3.1\", request_timeout=420.0)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Let's run one query\n",
    "response = query_engine.query(\"Give me the names of Apache spark components in bullets\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
