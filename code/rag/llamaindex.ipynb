{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Llama 3.1 8b Locally on Windows with LlamaIndex: A Practical Guide\n",
    "\n",
    "In this blog post, we'll explore how to run LlamaIndex with the Llama 3.1 8B parameters locally on a Windows machine. LlamaIndex is a powerful tool for integrating large language models with external data sources, and we will demonstrate how to leverage a PDF document and a vectorstore for efficient information retrieval. Whether you're new to LlamaIndex or looking for a practical guide to running Llama locally, this post will walk you through the steps to set up and harness the capabilities of these advanced models for your own projects.\n",
    "\n",
    "\n",
    "#### Setting up LLama 3.1 8b model locally on Windows\n",
    "\n",
    "1. [Download](https://ollama.com/download/windows) installable for Windows\n",
    "2. [Search](https://ollama.com/search) llama 3.1 and copy the command from model page\n",
    "3. Run the command on Windows cli\n",
    "    ollama run llama3.1\n",
    "\n",
    "4. Once up and running \n",
    "\n",
    "![Llama 3.1](./llama3.1.PNG)\n",
    "\n",
    "5. Or, hit http://localhost:11434/\n",
    "\n",
    "    The browser shows \"Ollama is running\"\n",
    "\n",
    "\n",
    "NOTE: I am running this on i9 Intel vPRO with 64.0 GB with NVIDIA RTX 3500 ADA (12 GB dedicated RAM). You can try tiny llama in case of resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_page_range(pdf_path, out_path, start_page, end_page):\n",
    "    \"\"\"\n",
    "    Extracts text from a range of pages from a PDF file using PyMuPDF.\n",
    "    \"\"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "\n",
    "    # Create a new PDF for the output\n",
    "    output_document = fitz.open()\n",
    "    text = \"\"\n",
    "    # Extract pages from the specified range\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page = document.load_page(page_num)\n",
    "        output_document.insert_pdf(document, from_page=page_num, to_page=page_num)\n",
    "\n",
    "    # Save the extracted pages to the output file\n",
    "    output_document.save(out_path)\n",
    "\n",
    "    print(f\"Pages {start_page + 1} to {end_page + 1} have been extracted and saved to '{out_path}'.\")\n",
    "\n",
    "\n",
    "def parse_pdf_sections(document):\n",
    "    parsed_dict = {}\n",
    "    main_section_key = \"\"\n",
    "    section_key = \"\"\n",
    "    for page in range(document.page_count):\n",
    "        #print(f\"Page {page + 1}\")\n",
    "        text_dict = document[page].get_text(\"dict\")\n",
    "\n",
    "        for block in text_dict.get(\"blocks\", []):\n",
    "            #print(f\"Block {block['number']}\")\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    #print(span)\n",
    "                    text = span.get(\"text\", \"\").strip()\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        if not text.isnumeric() and text.strip().lower() not in [\"Chapter 1: Introduction to Apache Spark: A Unified Analytics Engine\".lower(), \"CHAPTER 1\".lower()]:\n",
    "                            #print(text)\n",
    "                            font_size = span.get(\"size\", 0)\n",
    "                            if font_size > 17 and font_size < 19:\n",
    "                                #print(text) \n",
    "                                main_section_key = text\n",
    "                                parsed_dict[main_section_key] = {}\n",
    "                                section_key = \"\"\n",
    "                                sub_section_key = \"\"\n",
    "                            \n",
    "                            if font_size > 15 and font_size < 16:\n",
    "                                #print(\"\\t\", text)\n",
    "                                section_key = text\n",
    "                                sub_section_key = \"\"\n",
    "                                if main_section_key:\n",
    "                                    parsed_dict[main_section_key][section_key] = {}\n",
    "\n",
    "                            if font_size > 11 and font_size < 12:\n",
    "                                #print(\"\\t\\t\", text)\n",
    "                                sub_section_key = text\n",
    "                                if main_section_key and section_key:\n",
    "                                    parsed_dict[main_section_key][section_key][sub_section_key] = []\n",
    "\n",
    "                            if font_size > 10 and font_size < 11:\n",
    "                                #print(\"\\t\\t\\t\", text)\n",
    "                                if main_section_key and section_key and sub_section_key:\n",
    "                                    parsed_dict[main_section_key][section_key][sub_section_key].append(text)\n",
    "                                \n",
    "                                if main_section_key and section_key and not sub_section_key:\n",
    "                                    sub_section_key = \"content\"\n",
    "                                    parsed_dict[main_section_key][section_key][sub_section_key] = [text]\n",
    "\n",
    "    for main_section_key in parsed_dict:\n",
    "        for section_key in parsed_dict[main_section_key]:\n",
    "            for sub_section_key in parsed_dict[main_section_key][section_key]:\n",
    "                content = parsed_dict[main_section_key][section_key][sub_section_key]\n",
    "                parsed_dict[main_section_key][section_key][sub_section_key] = \" \".join(content)\n",
    "                \n",
    "    return parsed_dict\n",
    "\n",
    "\n",
    "def fixed_size_chunking(text, metadata, chunk_size, overlap, char=False):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks of a fixed size with optional overlap.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be chunked.\n",
    "    chunk_size (int): The size of each chunk.\n",
    "    overlap (int): The number of overlapping elements between consecutive chunks.\n",
    "    char (bool): If True, chunk by characters. If False, chunk by words. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    if char:\n",
    "        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]\n",
    "    else:\n",
    "        text = text.split()\n",
    "        return [ metadata + text[i:i+chunk_size] for i in range(0, len(text) - len(metadata), chunk_size - overlap)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf preprocessing\n",
    "\n",
    "#### Get the sample pages from Pdf\n",
    "\n",
    "I have taken Learning Spark 2.0 pdf and extracted only one chapter for this post. The function ``` get_page_range ``` extracts a page range and save it to a output pdf. In this case i have already created ``` sample.pdf ```. use the following code snippet to get the sample pdf.\n",
    "\n",
    "```python\n",
    "text = get_page_range(\"./LearningSpark2.0.pdf\", \"sample.pdf\", 24, 41)\n",
    "```\n",
    "\n",
    "#### Extract text from pdf\n",
    "\n",
    "I have written small parsing logic to keep the text for a section together to preserve the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = fitz.open(\"./sample.pdf\")\n",
    "parsed_content = parse_pdf_sections(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Chunking the content\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for main_section_key, main_section_value in parsed_content.items():\n",
    "    for section_key, section_value in main_section_value.items():\n",
    "        for sub_section_key, sub_section_value in section_value.items():\n",
    "            metadata = (main_section_key + \" \" + section_key + \" \" + sub_section_key).strip().split()\n",
    "            chnk = fixed_size_chunking(sub_section_value, metadata, 1000, 200, char=False)\n",
    "            txt_chnk = [' '.join(c) for c in chnk]\n",
    "            chunks.extend(txt_chnk)\n",
    "            lowercased_list = [str(item).lower() for item in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# Settings\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "index = VectorStoreIndex([])\n",
    "for chunk in lowercased_list:\n",
    "    index.insert(Document(text=chunk, extra_info={}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Spark SQL\n",
      "• Spark MLlib\n",
      "• Spark Structured Streaming\n",
      "• GraphX\n"
     ]
    }
   ],
   "source": [
    "# Use locally running Ollama Server for querying the index\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model = \"llama3.1\", request_timeout=420.0)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Let's run one query\n",
    "response = query_engine.query(\"Give me the names of Apache spark components in bullets\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
