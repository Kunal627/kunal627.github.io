{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoPE (Rotatory Positional Encoding)\n",
    "-----------------------------------\n",
    "\n",
    "The sequence of token plays a a crucial role in NLP tasks like generation and translation. Positional encoding is used to enable transformers to understand the order of words in a sequence. \"Attention is all you need\" paper by Vaswani et al. suggested sinusoidal based functions to assign a unique position to a token in a sequence.\n",
    "\n",
    "RoPE is a more advanced form of positional encoding that’s particularly useful for handling relative positions and long-range dependencies. It better integrates with the self-attention mechanism, improving performance in models that need to capture complex or long-term relationships, such as those in large-scale natural language processing tasks. This makes it a preferred alternative over traditional sinusoidal encoding in many modern transformer architectures.\n",
    "\n",
    "In RoPE, positional encoding is implemented by rotating token embeddings in the vector space. The rotation ensures that the relative positional information between tokens is preserved. This means \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little math behind the rotation matrix. \n",
    "\n",
    "In following method P is rotated by an angle theta (θ) in counter clockwise direction.  A little math to understand how rotation matrix is derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](../images/rotationmatrix.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotary Positional Encoding (RoPE) is designed to incorporate positional information into embeddings in a way that preserves the relationships between tokens. The reason RoPE uses multiplication instead of addition can be understood through several key points:\n",
    "\n",
    "##### Preservation of Relative Positional Information\n",
    "* Multiplication allows the positional information to be integrated in a way that maintains the relative distance between tokens. By rotating the query and key vectors based on their positions, RoPE preserves the relationships between them in a continuous manner.\n",
    "\n",
    "* Addition, on the other hand, would simply shift the embeddings without preserving their relative angles and magnitudes, which could lead to loss of important relational information.\n",
    "\n",
    "##### Geometric Interpretation\n",
    "* The rotation (via multiplication) can be viewed as a geometric transformation in which the vector's direction is changed without altering its length. This is particularly useful in maintaining the attention mechanism’s sensitivity to the positions of tokens.\n",
    "\n",
    "* In contrast, adding a positional encoding directly to the embeddings would create a fixed shift in their representation, distorting their inherent relationships and potentially complicating the attention mechanism.\n",
    "\n",
    "##### Use of Orthogonal Transformations\n",
    "* RoPE employs rotation matrices that are orthogonal. When you multiply a vector by an orthogonal matrix (like a rotation matrix), the result preserves the length of the vector, maintaining the magnitude and relative positioning of embeddings.\n",
    "\n",
    "* This property ensures that the embeddings retain their original information while also incorporating positional context effectively.\n",
    "\n",
    "##### Compatibility with Attention Mechanism\n",
    "* The self-attention mechanism relies heavily on dot products between query and key vectors. When RoPE applies rotation (multiplication), it maintains the mathematical properties needed for effective attention scoring.\n",
    "\n",
    "* Using addition would change the distribution of the embeddings, potentially leading to suboptimal attention scores and reducing the model's overall effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE implementation (source Roformer paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](../images/ropeencoding.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is just for illustration purposes, for the sake of simplicity we are using 2 embedding dimensions\n",
    "\n",
    "from common import *\n",
    "np.random.seed(42)\n",
    "text = \"there is a cat there is a cat\"\n",
    "n_embed = 2  # number of embedding dimensions\n",
    "tokens = text.lower().split() \n",
    "seq_len = len(tokens)  \n",
    "vocab = sorted(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "np.random.seed(42)\n",
    "embedding_matrix = np.random.rand(vocab_size, n_embed).round(3)\n",
    "embedding_matrix = np.random.uniform(low=0.0, high=1.0, size=(vocab_size, n_embed))\n",
    "tok2pos = {tok: i for i, tok in enumerate(vocab)}  # token to position mapping in vocab\n",
    "pos2token =   {i: tok for i, tok in enumerate(tok2pos)} # position to token mapping in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = get_encoding(embedding_matrix, tokens, tok2pos)\n",
    "seq_encoding = np.array([encoding[token] for token in tokens]).round(3) # get the encoding of the sequence\n",
    "rope_encoding = get_rope_encoding(seq_encoding, n_embed, seq_len) # apply rope encoding to the sequence\n",
    "out = rope_encoding.reshape(seq_len, n_embed) # reshape to seq_len x n_embed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8142155015563441 0.7241382159408638\n",
      "1.1898447977996685 0.8032994460349141\n"
     ]
    }
   ],
   "source": [
    "token1 = out[0] #there\n",
    "token2 = out[3] #cat\n",
    "cs_rope = cosine_similarity(token1, token2)                     # cosine similarity between there and cat after rope encoding\n",
    "cs_embed = cosine_similarity(seq_encoding[0], seq_encoding[3])  # cosine similarity between there and cat before rope encoding\n",
    "ed_rope = euclidean_distance(token1, token2)                    # euclidean distance between there and cat after rope encoding\n",
    "ed_embed = euclidean_distance(seq_encoding[0], seq_encoding[3]) # euclidean distance between there and cat before rope encoding\n",
    "\n",
    "print(cs_rope, cs_embed)\n",
    "print(ed_rope, ed_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotary Positional Encoding (RoPE) changes the relative positions of tokens in the embedding space. This results in different distances and similarities between tokens compared to their original embeddings, capturing positional information and preserving the rotational invariance for attention-based models.\n",
    "\n",
    "\n",
    "#### References and further reading\n",
    "\n",
    "* RoFormer: Enhanced Transformer with Rotary Position Embedding https://arxiv.org/abs/2104.09864\n",
    "* https://github.com/adalkiran/llama-nuts-and-bolts/blob/main/docs/10-ROPE-ROTARY-POSITIONAL-EMBEDDINGS.md\n",
    "* Pytorch implementation - https://pytorch.org/torchtune/stable/_modules/torchtune/modules/position_embeddings.html#RotaryPositionalEmbeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
