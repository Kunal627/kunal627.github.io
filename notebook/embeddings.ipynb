{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demistifying embeddings\n",
    "======================\n",
    "\n",
    "Embeddings is one of the core concept in NLP. It's a way to represent text into dense and low-dimensional vector space. Why low dimension? We will see later in this post.\n",
    "\n",
    "## Encoding vs Embeddings:\n",
    "Machines only understand the language of bits and bytes represented by binary number system 1 and 0 (base 2). All forms of data is finally stored in sequences of 0's and 1's. There are multiple encoding schemes (UTF-8, ISO-8859 etc.) to represent characters in binary format. Where encoding is a way to represent textual data in the form which computers understand, Embeddings on other hand is a dense vector representation of data (words, sentences, images). \n",
    "Embeddings aim to capture semantic similarities and relationships in a lower-dimensional space. They enable models to understand the context and meaning of words or phrases beyond simple character representation. \n",
    "Both encodings and embeddings play crucial roles in text processing and machine learning. While encodings ensure that text is represented in a binary format for storage and transmission, embeddings enable models to understand and leverage the semantic relationships between words or sentences. Understanding the differences and interplay between these two concepts is essential for effectively working with text data in NLP and other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII value of h:::  104\n",
      "ASCII value of e:::  101\n",
      "ASCII value of l:::  108\n",
      "ASCII value of l:::  108\n",
      "ASCII value of o:::  111\n",
      "Embedding of h:::  [0.3   0.559]\n",
      "Embedding of e:::  [0.716 0.37 ]\n",
      "Embedding of l:::  [0.135 0.508]\n",
      "Embedding of l:::  [0.5   0.067]\n",
      "Embedding of o:::  [0.959 0.731]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text = \"hello\"\n",
    "\n",
    "# Encoding the text ASCII\n",
    "for ch in text:\n",
    "    print(f\"ASCII value of {ch}::: \", ord(ch))\n",
    "\n",
    "\n",
    "# embeddings can be pre-trained or learned during the training of the model.\n",
    "# In real world, embeddings are represented in 100s of dimensions\n",
    "# Just for an example, character in 2-D space\n",
    "\n",
    "for ch in text:\n",
    "    print(f\"Embedding of {ch}::: \", np.random.rand(2).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to mention, embedding dimensions are abstract in the sense that they don't have human interpretable meaning. The dimensions encode complex interactions learned by the model. \n",
    "\n",
    "For example, the embedding for the word \"king\" might be a n-dimensional vector like [0.25, -0.34, 0.91, ...]. Each number in this vector contributes to the overall meaning of \"king\" but does not correspond to a single, easily interpretable feature.\n",
    "\n",
    "#### Naive Integer Embedding\n",
    "\n",
    "A naive way to represent text tokens with integers is by using a simple integer encoding or tokenization approach. This involves assigning a unique integer to each word (or token) in your vocabulary. It’s straightforward but lacks the nuance of more sophisticated methods like word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "\n",
      "Embeddings of words in the text\n",
      "Embedding of a:::  0\n",
      "Embedding of quick:::  1\n",
      "Embedding of brown:::  2\n",
      "Embedding of fox:::  3\n",
      "Embedding of jumps:::  4\n",
      "Embedding of over:::  5\n",
      "Embedding of the:::  6\n",
      "Embedding of lazy:::  7\n",
      "Embedding of dog:::  8\n",
      "\n",
      "Encoded text:::  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "text = \"a quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# tokenize the text\n",
    "tokenized_text = text.split()\n",
    "print(tokenized_text)\n",
    "print(\"\")\n",
    "\n",
    "# create a dictionary of words and their embeddings\n",
    "# use simple integer values as embeddings\n",
    "\n",
    "vocab = {word: i for i, word in enumerate(tokenized_text)}\n",
    "print(\"Embeddings of words in the text\")\n",
    "for word in tokenized_text:\n",
    "    print(f\"Embedding of {word}::: \", vocab[word])\n",
    "\n",
    "# print encoded text\n",
    "print(\"\")\n",
    "encoded_text = [vocab[word] for word in tokenized_text]\n",
    "print(\"Encoded text::: \", encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of Integer encoding:\n",
    "\n",
    "* The numbers are arbitrary and do not represent a relationships between words in the sentence. The semantics are lost with this embedding scheme\n",
    "* For out of vocabulary words, there is no integer representation. (Words which are not there in the training data)\n",
    "* The naive integer representation lacks the dense and distributed representation that embeddings provide, leading to worse performance in machine learning models.\n",
    "\n",
    "\n",
    "#### One-Hot Encoding\n",
    "Instead of assigning a unique integer, you can create a sparse one-hot encoded vector for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of a         :::  [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Embedding of brown     :::  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Embedding of dog       :::  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Embedding of fox       :::  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Embedding of jumps     :::  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Embedding of lazy      :::  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Embedding of over      :::  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Embedding of quick     :::  [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Embedding of the       :::  [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# sort the tokens and create a vocabulary\n",
    "# generate one hot encoding for each token\n",
    "\n",
    "sorted_tokens = sorted(set(tokenized_text))\n",
    "vocab = {word: i for i, word in enumerate(sorted_tokens)}\n",
    "embeddings = np.eye(len(vocab)) # one hot encoding of the tokens (vocab_size x vocab_size)\n",
    "\n",
    "for word in sorted_tokens:\n",
    "    print(f\"Embedding of {word:<10}::: \", embeddings[vocab[word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of One-Hot encoding:\n",
    "\n",
    "* Dimensionality of embedding matrix grows with vocabulary. This leads to sparse vectors, which are memory-inefficient and computationally expensive\n",
    "* No semantic information, even though fox and dog are semantically similar (animals), they have completely different vectors. This prevents models from understanding relationships like synonyms, antonyms, or words that often appear together.\n",
    "* One-hot encoding is based on a fixed vocabulary. If you encounter a word that wasn’t in your vocabulary during training (an out-of-vocabulary word), there is no way to represent it.This limits the generalization of models in real-world scenarios.\n",
    "\n",
    "\n",
    "#### Word Embeddings\n",
    "Word embeddings address most of the limitations of one-hot encoding\n",
    "* Low Dimensionality: Word embeddings reduce high-dimensional one-hot vectors to lower-dimensional dense vectors (e.g., 100 to 300 dimensions).\n",
    "* Semantic Meaning: Embeddings capture semantic relationships between words, placing similar words (e.g., \"fox\" and \"dog\") closer in the embedding space.\n",
    "* Contextual Information: Embedding models like BERT and GPT can capture context, meaning that the same word can have different embeddings depending on its context.\n",
    "* Better Generalization: Word embeddings generalize better because they capture word similarity and relationships in their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"a quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get token IDs and attention mask\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Get the embeddings from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# The outputs contain the last hidden states\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# Get embeddings for all tokens\n",
    "token_embeddings = last_hidden_states[0]\n",
    "\n",
    "embed_dict = {}\n",
    "\n",
    "# Get the embeddings for only fiox and dog\n",
    "\n",
    "for token, embedding in zip(input_tokens, token_embeddings):\n",
    "    if token in [\"fox\", \"dog\"]:\n",
    "        embed_dict[token] = embedding\n",
    "\n",
    "# shape of the embeddings\n",
    "embed_dict[\"fox\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between fox and dog:::  [[0.5633921]]\n"
     ]
    }
   ],
   "source": [
    "# calculate similarity between  fox and dog\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(embed_dict[\"fox\"].reshape(1, -1), embed_dict[\"dog\"].reshape(1, -1))\n",
    "print(\"Similarity between fox and dog::: \", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
