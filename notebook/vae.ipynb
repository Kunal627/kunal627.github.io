{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Variational Autoencoder (VAE) on the MNIST Dataset\n",
    "\n",
    "In this blog post, we'll explore how to train a Variational Autoencoder (VAE) to generate synthetic data using the MNIST dataset. We'll guide you through setting up the environment, defining and training the VAE model, and generating new images. These synthetic images will be used in addition to the original MNIST data to enhance a classification task. We'll also visualize the learned latent space and evaluate the quality of the generated images using the Fréchet Inception Distance (FID) score. \n",
    "\n",
    "Variational Autoencoders (VAEs) are generative models that learn to represent data in a lower-dimensional latent space while being able to reconstruct the original data from this space. Unlike traditional autoencoders, VAEs introduce a probabilistic approach by learning a distribution over the latent variables, allowing for the generation of new, similar data by sampling from this distribution. This makes VAEs particularly useful for tasks such as data synthesis, anomaly detection, and creating smooth interpolations between different data points. Their probabilistic nature provides a principled way to generate realistic and diverse synthetic data.\n",
    "\n",
    "\n",
    "#### Setting Up the Environment and Loading the Dataset\n",
    "First, we need to set up our environment and load the MNIST dataset. We will use PyTorch for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "latent_dim = 30\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the VAE Model\n",
    "Next, we define the VAE model. The VAE consists of an encoder and a decoder. The encoder maps the input data to a latent space, and the decoder reconstructs the data from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 400)\n",
    "        self.fc2_mean = nn.Linear(400, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(400, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, 400)\n",
    "        self.fc4 = nn.Linear(400, 28*28)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x.view(-1, 28*28))\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decode(z), mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the VAE\n",
    "We will now train the VAE using the MNIST dataset. The training loop involves forward and backward passes, and we will save checkpoints at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mean, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mean, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Images Using the Trained VAE\n",
    "After training the VAE, we can generate new images by sampling from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images(model, num_images=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, model.fc2_mean.in_features)\n",
    "        generated = model.decode(z).view(-1, 1, 28, 28).cpu().numpy()\n",
    "        fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(generated[i][0], cmap='gray')\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "plt = generate_images(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Synthetic images](../images/fake_images.png)\n",
    "\n",
    "#### Visualizing the Latent Space\n",
    "We can also visualize the latent space using t-SNE to reduce the dimensionality to 2D. \n",
    "\n",
    "Visualizing the latent space of a Variational Autoencoder (VAE) provides insights into how the model organizes and represents different features of the data. By mapping similar images to nearby points in the latent space, we can observe clusters that correspond to distinct characteristics, such as digit shapes in the MNIST dataset. The smooth transitions between points in the latent space suggest that the model has learned meaningful features, enabling it to generate realistic interpolations. This visualization can reveal how well the VAE captures the underlying data distribution, highlighting areas where the model performs well or struggles to differentiate certain features.\n",
    "\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a powerful dimensionality reduction technique commonly used for visualizing high-dimensional data, especially in the context of machine learning. Here are some key reasons to use t-SNE:\n",
    "\n",
    "* t-SNE is particularly effective for visualizing high-dimensional data in two or three dimensions. It preserves local structure, meaning that similar data points remain close together in the lower-dimensional representation.\n",
    "* Unlike linear techniques such as PCA (Principal Component Analysis), t-SNE is adept at capturing complex, non-linear relationships in the data, making it well-suited for data with intricate structures.\n",
    "* t-SNE can help reveal clusters or groupings within the data, allowing researchers to identify distinct classes or patterns that may not be apparent in the original high-dimensional space.\n",
    "* The resulting 2D or 3D visualizations from t-SNE are often more interpretable, enabling easier communication of insights to non-technical stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def get_latent_space(model, data_loader):\n",
    "    latent_vectors = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            mu, log_var = model.encode(inputs.view(-1, 28*28))\n",
    "            z = model.reparameterize(mu, log_var)\n",
    "            latent_vectors.append(z)\n",
    "    return torch.cat(latent_vectors, dim=0).cpu().numpy()\n",
    "\n",
    "def visualize_latent_space(latent_vectors):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latent_2d = tsne.fit_transform(latent_vectors)\n",
    "    plt.scatter(latent_2d[:, 0], latent_2d[:, 1], alpha=0.7)\n",
    "    plt.title(\"2D t-SNE visualization of VAE latent space\")\n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "latent_vectors = get_latent_space(model, val_loader)\n",
    "visualize_latent_space(latent_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../images/latent_space.png)\n",
    "\n",
    "#### Calculating the FID Score\n",
    "The Fréchet Inception Distance (FID) score is a metric used to evaluate the quality of generated images by comparing the distribution of generated images to the distribution of real images. A lower FID score indicates that the generated images are more similar to the real images. Achieved FID of 3.58 (on 1024 generated images) and FID score: 4.94783878326416 on MNIST dataset.\n",
    "\n",
    "#### Why FID?\n",
    "\n",
    "* FID compares the distribution of generated images to the distribution of real images in feature space, rather than just pixel values. This allows for a more meaningful evaluation of visual quality.\n",
    "*  FID has been shown to correlate well with human judgment on the quality of generated images, making it a more perceptually relevant metric than pixel-wise comparisons like Mean Squared Error (MSE).\n",
    "* FID measures both the quality and diversity of generated images. A lower FID score indicates that the generated images are not only similar to real images but also cover a diverse range of styles and classes.\n",
    "\n",
    "#### Interpreting a FID Score of 3.58\n",
    "FID scores typically range from 0 (perfect similarity) to higher values indicating worse performance. In practice, FID scores below 10 are often considered to indicate good image quality, while scores above 20 may indicate poorer performance.\n",
    "\n",
    "A FID score of 3.58 is quite low and suggests that the generated images are of high quality and closely resemble the real images from the dataset. It indicates that the model has effectively learned the underlying distribution of the data and can generate diverse and realistic images.\n",
    "\n",
    "#### Context Matters\n",
    "The interpretation of FID scores can depend on the dataset being used. For example, achieving a low FID score on a simple dataset like MNIST might be easier than on more complex datasets like CIFAR-10 or ImageNet.\n",
    "\n",
    "When comparing models, it’s important to look at relative FID scores (e.g., Model A vs. Model B) rather than absolute scores, as the significance of the score can vary with different datasets and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from modeldef import ConvVAE\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 30  # Dimension of the latent space\n",
    "batch_size = 256\n",
    "# load the checkpoint\n",
    "ckpt = 'vae_checkpoint.pth_epoch_81.pth'\n",
    "\n",
    "checkpoint = torch.load(f'./checkpoints/{ckpt}')\n",
    "model = ConvVAE(latent_dim=latent_dim).to(device)\n",
    "\n",
    "# Transformations for MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(3), # Convert to 3 channels\n",
    "    transforms.Resize(299),  # Resize to 299x299 (Inception-v3 input size)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.to(torch.uint8))\n",
    "#    transforms.Lambda(lambda x: x.expand(3, -1, -1)),\n",
    "#    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize to [-1, 1] range,\n",
    "#    transforms.Lambda(lambda x: (x * 255).clamp(0, 255).to(torch.uint8))\n",
    "])\n",
    "\n",
    "transform_gen = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.expand(3, -1, -1)),\n",
    "    transforms.Resize(299),\n",
    "    transforms.Lambda(lambda x: x.to(torch.uint8))\n",
    "#    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#    transforms.Lambda(lambda x: (x * 255).clamp(0, 255).to(torch.uint8))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(mnist_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "\n",
    "# Extract features from real images and random noise (as \"fake\" images)\n",
    "for real_images, _ in dataloader:\n",
    "    real_images = real_images.to(device)\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(batch_size, model.fc2.in_features)  # Sample from the latent space\n",
    "        generated = model.decode(z).view(-1, 1, 28, 28).cpu()  # Generate images from the latent vectors\n",
    "        fake_images = torch.stack([transform_gen(img) for img in generated])\n",
    "    # Update the FID metric with real and fake images\n",
    "    fid.update(real_images, real=True)\n",
    "    fid.update(fake_images, real=False)\n",
    "\n",
    "# Compute the FID score\n",
    "fid_score = fid.compute().item()\n",
    "print(f\"FID score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, we have covered the process of training a VAE on the MNIST dataset, generating new images, visualizing the latent space, and calculating the FID score to evaluate the quality of generated images. The generated images can be used in addition to the MNIST data for various classification tasks. The VAE model provides a powerful way to learn meaningful representations of the data, which can be useful in many machine learning applications.\n",
    "\n",
    "#### Common Gotchas Leading to High FID Scores\n",
    "\n",
    "Initially I got a very high FID score (334) while working through this example. Even though synthetic images looked OK. The issue was with the way preprocessing transformations were applied before FID calculation.\n",
    "\n",
    "* Ensure that the preprocessing applied to the training data is the same as that applied to the generated images. Differences in scaling, normalization, or augmentation can lead to discrepancies that inflate the FID score.\n",
    "\n",
    "* When converting images to the correct format, ensure that they are normalized appropriately. For example, if the training images are scaled to the range [0, 1], the generated images should be treated similarly. Mismatched normalization can create significant differences in the data distributions.\n",
    "\n",
    "* Applying different transformations (e.g., resizing, cropping, or color adjustments) to real and generated images can distort their distributions. For example, if you use Grayscale(3) for generated images but not for the training images, it could lead to higher FID scores.\n",
    "\n",
    "* If the VAE's latent space is not well-learned, it may produce unrealistic samples that don't capture the diversity of the training data. This could lead to a high FID score despite visually similar images.\n",
    "\n",
    "\n",
    "#### Additional reading/references\n",
    "\n",
    "* Foundations and Trends in Machine Learning - An Introduction to Variational Autoencoders https://arxiv.org/pdf/1906.02691\n",
    "* Working Code - [Visit Github](https://github.com/Kunal627/kunal627.github.io/tree/main/code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
