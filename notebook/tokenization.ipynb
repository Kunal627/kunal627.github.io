{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "1. White space tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteSpaceTokenizer:\n",
    "    def __init__(self, lower_case=True, remove_punctuation=True):\n",
    "        self.whitespace = [' ', '\\t', '\\n', '\\r']\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.vocabulary = {}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if self.lower_case:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = ''.join([c for c in text if c.isalnum() or c in self.whitespace])\n",
    "        \n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        unique_tokens = set()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            unique_tokens.update(tokens)\n",
    "        print(unique_tokens)\n",
    "        self.vocabulary = {token: i for i, token in enumerate(unique_tokens)}\n",
    "        self.vocabulary['<UNK>'] = len(self.vocabulary)\n",
    "        self.idx2token = {i: token for token, i in self.vocabulary.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, self.vocabulary['<UNK>']) for token in tokens if token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return [self.idx2token[i] for i in indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'this', 'is', 'a', 'test']\n",
      "{'a', 'hello', 'this', 'is', 'test', 'world'}\n",
      "{'a': 0, 'hello': 1, 'this': 2, 'is': 3, 'test': 4, 'world': 5, '<UNK>': 6}\n",
      "[1, 6, 6, 6]\n",
      "['hello', '<UNK>', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! This is a test.\"\n",
    "tokenizer = WhiteSpaceTokenizer(remove_punctuation=True)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "tokenizer.build_vocabulary([text])\n",
    "print(tokenizer.vocabulary)\n",
    "encoded = tokenizer.encode(\"Hello I am there\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, lower_case=True, remove_punctuation=True):\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.vocabulary = {}\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        if self.lower_case:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = ''.join([c for c in text if c.isalnum()])\n",
    "        return list(text)\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        unique_tokens = set()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            unique_tokens.update(tokens)\n",
    "        self.vocabulary = {token: i for i, token in enumerate(unique_tokens)}\n",
    "        self.vocabulary['<UNK>'] = len(self.vocabulary)\n",
    "        self.idx2token = {i: token for token, i in self.vocabulary.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, self.vocabulary['<UNK>']) for token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return [self.idx2token[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', 't', 'h', 'i', 's', 'i', 's', 'a', 't', 'e', 's', 't']\n",
      "{'d': 0, 'w': 1, 'h': 2, 'i': 3, 'e': 4, 'o': 5, 'l': 6, 'a': 7, 's': 8, 't': 9, 'r': 10, '<UNK>': 11}\n",
      "[2, 4, 6, 6, 5, 3, 7, 11, 9, 2, 4, 10, 4]\n",
      "['h', 'e', 'l', 'l', 'o', 'i', 'a', '<UNK>', 't', 'h', 'e', 'r', 'e']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! This is a test.\"\n",
    "tokenizer = CharTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "tokenizer.build_vocabulary([text])\n",
    "print(tokenizer.vocabulary)\n",
    "encoded = tokenizer.encode(\"Hello I am there\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramTokenization:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.vocabulary = {}\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [text[i:i+self.n] for i in range(len(text)-self.n+1)]\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        unique_tokens = set()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            unique_tokens.update(tokens)\n",
    "        self.vocabulary = {token: i for i, token in enumerate(unique_tokens)}\n",
    "        self.vocabulary['<UNK>'] = len(self.vocabulary)\n",
    "        self.idx2token = {i: token for token, i in self.vocabulary.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, self.vocabulary['<UNK>']) for token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return [self.idx2token[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'ello', 'llo,', 'lo, ', 'o, w', ', wo', ' wor', 'worl', 'orld', 'rld!', 'ld! ', 'd! T', '! Th', ' Thi', 'This', 'his ', 'is i', 's is', ' is ', 'is a', 's a ', ' a t', 'a te', ' tes', 'test', 'est.']\n",
      "{' tes': 0, 'worl': 1, 'a te': 2, 'test': 3, 'rld!': 4, 'o, w': 5, 'This': 6, 'is i': 7, 'ld! ': 8, 'est.': 9, ' Thi': 10, ' a t': 11, 'llo,': 12, 'lo, ': 13, 'd! T': 14, ', wo': 15, ' wor': 16, 'Hell': 17, 'is a': 18, 'ello': 19, 'his ': 20, '! Th': 21, 'orld': 22, 's a ': 23, ' is ': 24, 's is': 25, '<UNK>': 26}\n",
      "[17, 19, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
      "['Hell', 'ello', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! This is a test.\"\n",
    "tokenizer = NgramTokenization(4)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "tokenizer.build_vocabulary([text])\n",
    "print(tokenizer.vocabulary)\n",
    "encoded = tokenizer.encode(\"Hello I am there\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']\n",
      "{',': 0, 'a': 1, 'hello': 2, 'this': 3, '!': 4, 'is': 5, 'test': 6, 'world': 7, '.': 8, '<UNK>': 9}\n",
      "[2, 9, 9, 9]\n",
      "['hello', '<UNK>', '<UNK>', '<UNK>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chandk10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize, TreebankWordDetokenizer, RegexpTokenizer, WordPunctTokenizer\n",
    "\n",
    "class WordTokenizer:\n",
    "    def __init__(self, tokenizer = 'punkt'):\n",
    "        self.supported_tokenizers = [\"punkt\", \"treebank\", \"wordpunct\", \"regexp\", \"whitespace\"]\n",
    "        assert tokenizer in self.supported_tokenizers, f\"Unsupported tokenizer: {tokenizer}\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # Tokenization using nltk's word_tokenize\n",
    "    def tokenize(self, text):\n",
    "        if self.tokenizer == 'punkt':\n",
    "            tokens = word_tokenize(text.lower())\n",
    "        elif self.tokenizer == 'regexp':\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokens = tokenizer.tokenize(text.lower())\n",
    "        elif self.tokenizer == 'treebank':\n",
    "            tokenizer = TreebankWordDetokenizer()\n",
    "            tokens = tokenizer.tokenize(text.lower())\n",
    "        elif self.tokenizer == 'wordpunct':\n",
    "            tokenizer = WordPunctTokenizer()\n",
    "            tokens = tokenizer.tokenize(text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def build_vocabulary(self, texts):\n",
    "        unique_tokens = set()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            unique_tokens.update(tokens)\n",
    "        self.vocabulary = {token: i for i, token in enumerate(unique_tokens)}\n",
    "        self.vocabulary['<UNK>'] = len(self.vocabulary)\n",
    "        self.idx2token = {i: token for token, i in self.vocabulary.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, self.vocabulary['<UNK>']) for token in tokens]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return [self.idx2token[i] for i in indices]\n",
    "\n",
    "\n",
    "text = \"Hello, world! This is a test.\"\n",
    "tokenizer = WordTokenizer(\"punkt\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "tokenizer.build_vocabulary([text])\n",
    "print(tokenizer.vocabulary)\n",
    "encoded = tokenizer.encode(\"hello I am there\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', \"'re\", 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(\"Hello, how're you doing today?\")\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
